{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dapr Agents: A Framework for Agentic AI Systems","text":"<p>Dapr Agents is a developer framework designed to build production-grade resilient AI agent systems that operate at scale. Built on top of the battle-tested Dapr project, it enables software developers to create AI agents that reason, act, and collaborate using Large Language Models (LLMs), while leveraging built-in observability and stateful workflow execution to guarantee agentic workflows complete successfully, no matter how complex.</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Scale and Efficiency: Run thousands of agents efficiently on a single core. Dapr distributes single and multi-agent apps transparently across fleets of machines and handles their lifecycle.</li> <li>Workflow Resilience: Automatically retries agentic workflows and ensures task completion.</li> <li>Kubernetes-Native: Easily deploy and manage agents in Kubernetes environments.</li> <li>Data-Driven Agents: Directly integrate with databases, documents, and unstructured data by connecting to dozens of different data sources.</li> <li>Multi-Agent Systems: Secure and observable by default, enabling collaboration between agents.</li> <li>Vendor-Neutral &amp; Open Source: Avoid vendor lock-in and gain flexibility across cloud and on-premises deployments.</li> <li>Platform-Ready: Built-in RBAC, access scopes and declarative resources enable platform teams to integrate Dapr agents into their systems. </li> </ul>"},{"location":"#why-choose-dapr-agents","title":"Why Choose Dapr Agents?","text":""},{"location":"#scalable-workflows-as-a-first-class-citizen","title":"Scalable Workflows as a First Class Citizen","text":"<p>Dapr Agents uses a durable-execution workflow engine that guarantees each agent task executes to completion in the face of network interruptions, node crashes and other types of disruptive failures. Developers do not need to know about the underlying concepts of the workflow engine - simply write an agent that performs any number of tasks and these will get automatically distributed across the cluster. If any task fails, it will be retried and recover its state from where it left off.</p>"},{"location":"#cost-effective-ai-adoption","title":"Cost-Effective AI Adoption","text":"<p>Dapr Agents builds on top of Dapr's Workflow API, which under the hood represents each agent as an actor, a single unit of compute and state that is thread-safe and natively distributed, lending itself well to an agentic Scale-To-Zero architecture. This minimizes infrastructure costs, making AI adoption accessible to everyone. The underlying virtual actor model allows thousands of agents to run on demand on a single core machine with double-digit millisecond latency when scaling from zero. When unused, the agents are reclaimed by the system but retain their state until the next time they are needed. With this design, there's no trade-off between performance and resource efficiency.</p>"},{"location":"#data-centric-ai-agents","title":"Data-Centric AI Agents","text":"<p>With built-in connectivity to over 50 enterprise data sources, Dapr Agents efficiently handles structured and unstructured data. From basic PDF extraction to large-scale database interactions, it enables seamless data-driven AI workflows with minimal code changes. Dapr's bindings and state stores, along with MCP support, provide access to a large number of data sources that can be used to ingest data to an agent.</p>"},{"location":"#accelerated-development","title":"Accelerated Development","text":"<p>Dapr Agents provides a set of AI features that give developers a complete API surface to tackle common problems. Some of these include:</p> <ul> <li>Multi-agent communications</li> <li>Structured outputs</li> <li>Multiple LLM providers</li> <li>Contextual memory</li> <li>Flexible prompting</li> <li>Intelligent tool selection</li> <li>MCP integration</li> </ul>"},{"location":"#integrated-security-and-reliability","title":"Integrated Security and Reliability","text":"<p>By building on top of Dapr, platform and infrastructure teams can apply Dapr's resiliency policies to the database and/or message broker of their choice that are used by Dapr Agents. These policies include timeouts, retry/backoffs and circuit breakers. When it comes to security, Dapr provides the option to scope access to a given database or message broker to one or more agentic app deployments. In addition, Dapr Agents uses mTLS to encrypt the communication layer of its underlying components. </p>"},{"location":"#built-in-messaging-and-state-infrastructure","title":"Built-in Messaging and State Infrastructure","text":"<ul> <li>\ud83c\udfaf Service-to-Service Invocation: Facilitates direct communication between agents with built-in service discovery, error handling, and distributed tracing. Agents can leverage this for synchronous messaging in multi-agent workflows.</li> <li>\u26a1\ufe0f Publish and Subscribe: Supports loosely coupled collaboration between agents through a shared message bus. This enables real-time, event-driven interactions critical for task distribution and coordination.</li> <li>\ud83d\udd04 Durable Workflow: Defines long-running, persistent workflows that combine deterministic processes with LLM-based decision-making. Dapr Agents uses this to orchestrate complex multi-step agentic workflows seamlessly.</li> <li>\ud83e\udde0 State Management: Provides a flexible key-value store for agents to retain context across interactions, ensuring continuity and adaptability during workflows.</li> <li>\ud83e\udd16 Actors: Implements the Virtual Actor pattern, allowing agents to operate as self-contained, stateful units that handle messages sequentially. This eliminates concurrency concerns and enhances scalability in agentic systems.</li> </ul>"},{"location":"#vendor-neutral-and-open-source","title":"Vendor-Neutral and Open Source","text":"<p>As a part of CNCF, Dapr Agents is vendor-neutral, eliminating concerns about lock-in, intellectual property risks, or proprietary restrictions. Organizations gain full flexibility and control over their AI applications using open-source software they can audit and contribute to.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Set up in 2 minutes</p> <p>Install <code>Dapr Agents</code> with <code>pip</code> and set up your Dapr environment in minutes</p> <p> Installation</p> </li> <li> <p> Start experimenting</p> <p>Build your first agent and design a custom workflow to get started with Dapr Agents.</p> <p> Quickstarts</p> </li> <li> <p> Learn more</p> <p>Learn more about Dapr Agents and its main components!</p> <p> Concepts</p> </li> </ul>"},{"location":"concepts/agents/","title":"Agents","text":"<p>Agents in <code>Dapr Agents</code> are autonomous systems powered by Large Language Models (LLMs), designed to execute tasks, reason through problems, and collaborate within workflows. Acting as intelligent building blocks, agents seamlessly combine LLM-driven reasoning with tool integration, memory, and collaboration features to enable scalable, production-grade agentic systems.</p> <p></p>"},{"location":"concepts/agents/#core-features","title":"Core Features","text":""},{"location":"concepts/agents/#1-llm-integration","title":"1. LLM Integration","text":"<p>Dapr Agents provides a unified interface to connect with LLM inference APIs. This abstraction allows developers to seamlessly integrate their agents with cutting-edge language models for reasoning and decision-making.</p>"},{"location":"concepts/agents/#2-structured-outputs","title":"2. Structured Outputs","text":"<p>Agents in Dapr Agents leverage structured output capabilities, such as OpenAI\u2019s Function Calling, to generate predictable and reliable results. These outputs follow JSON Schema Draft 2020-12 and OpenAPI Specification v3.1.0 standards, enabling easy interoperability and tool integration.</p>"},{"location":"concepts/agents/#3-tool-selection","title":"3. Tool Selection","text":"<p>Agents dynamically select the appropriate tool for a given task, using LLMs to analyze requirements and choose the best action. This is supported directly through LLM parametric knowledge and enhanced by Function Calling, ensuring tools are invoked efficiently and accurately.</p>"},{"location":"concepts/agents/#4-mcp-support","title":"4. MCP Support","text":"<p>Dapr Agents includes built-in support for the Model Context Protocol (MCP), enabling agents to dynamically discover and invoke external tools through a standardized interface. Using the provided MCPClient, agents can connect to MCP servers via two transport options: stdio for local development and sse for remote or distributed environments.</p> <p>Once connected, the MCP client fetches all available tools from the server and prepares them for immediate use within the agent\u2019s toolset. This allows agents to incorporate capabilities exposed by external processes\u2014such as local Python scripts or remote services without hardcoding or preloading them. Agents can invoke these tools at runtime, expanding their behavior based on what\u2019s offered by the active MCP server.</p>"},{"location":"concepts/agents/#5-memory","title":"5. Memory","text":"<p>Agents retain context across interactions, enhancing their ability to provide coherent and adaptive responses. Memory options range from simple in-memory lists for managing chat history to vector databases for semantic search and retrieval. Dapr Agents also integrates with Dapr state stores, enabling scalable and persistent memory for advanced use cases from 28 different state store providers.</p>"},{"location":"concepts/agents/#6-prompt-flexibility","title":"6. Prompt Flexibility","text":"<p>Dapr Agents supports flexible prompt templates to shape agent behavior and reasoning. Users can define placeholders within prompts, enabling dynamic input of context for inference calls. By leveraging prompt formatting with Jinja templates, users can include loops, conditions, and variables, providing precise control over the structure and content of prompts. This flexibility ensures that LLM responses are tailored to the task at hand, offering modularity and adaptability for diverse use cases.</p>"},{"location":"concepts/agents/#7-agent-services","title":"7. Agent Services","text":"<p>Agents are exposed as independent services using FastAPI and Dapr applications. This modular approach separates the agent\u2019s logic from its service layer, enabling seamless reuse, deployment, and integration into multi-agent systems.</p>"},{"location":"concepts/agents/#8-message-driven-communication","title":"8. Message-Driven Communication","text":"<p>Agents collaborate through Pub/Sub messaging, enabling event-driven communication and task distribution. This message-driven architecture allows agents to work asynchronously, share updates, and respond to real-time events, ensuring effective collaboration in distributed systems.</p>"},{"location":"concepts/agents/#9-workflow-orchestration","title":"9. Workflow Orchestration","text":"<p>Dapr Agents supports both deterministic and event-driven workflows to manage multi-agent systems via Dapr Workflows. Deterministic workflows provide clear, repeatable processes, while event-driven workflows allow for dynamic, adaptive collaboration between agents in centralized or decentralized architectures.</p>"},{"location":"concepts/agents/#agent-types","title":"Agent Types","text":"<p>Dapr Agents provides two agent types, each designed for different use cases:</p>"},{"location":"concepts/agents/#agent","title":"Agent","text":"<p>The <code>Agent</code> class is a conversational agent that manages tool calls and conversations using a language model. It provides immediate, synchronous execution with built-in conversation memory and tool integration capabilities.</p> <p>Key Characteristics: - Synchronous execution with immediate responses - Built-in conversation memory and tool history tracking - Iterative conversation processing with max iteration limits - Direct tool execution and result processing - Graceful shutdown support with cancellation handling</p> <p>When to use: - Building conversational assistants that need immediate responses - Scenarios requiring real-time tool execution and conversation flow - When you need direct control over the conversation loop - Quick prototyping and development of agent interactions</p> <p>Example Usage: <pre><code>from dapr_agents import Agent\nfrom dapr_agents.llm.dapr import DaprChatClient\nfrom dapr_agents.memory import ConversationDaprStateMemory\n\n# Create an incident intake agent\nagent = Agent(\n    name=\"IncidentIntakeBot\",\n    role=\"Incident Reporting Assistant\",\n    instructions=[\n        \"Collect detailed operational incident information\",\n        \"Retain user inputs across sessions for audit and recovery\",\n        \"Use memory to guide follow-up questions based on previous input\",\n        \"Update incident records as new details are provided\",\n    ],\n    tools=[incident_lookup_tool, escalate_incident_tool, update_incident_tool],\n)\n\n# Conversation history is preserved across calls\nagent.run(\"first input\")\nagent.run(\"second input\")\n</code></pre></p>"},{"location":"concepts/agents/#durableagent","title":"DurableAgent","text":"<p>The <code>DurableAgent</code> class is a workflow-based agent that extends the standard Agent with Dapr Workflows for long-running, fault-tolerant, and durable execution. It provides persistent state management, automatic retry mechanisms, and deterministic execution across failures.</p> <p>Key Characteristics: - Workflow-based execution using Dapr Workflows - Persistent workflow state management across sessions and failures - Automatic retry and recovery mechanisms - Deterministic execution with checkpointing - Built-in message routing and agent communication - Supports complex orchestration patterns and multi-agent collaboration</p> <p>When to use: - Multi-step workflows that span time or systems - Tasks requiring guaranteed progress tracking and state persistence - Scenarios where operations may pause, fail, or need recovery without data loss - Complex agent orchestration and multi-agent collaboration - Production systems requiring fault tolerance and scalability</p> <p>Example Usage: <pre><code>from dapr_agents import DurableAgent\nfrom dapr_agents.llm.dapr import DaprChatClient\nfrom dapr_agents.memory import ConversationDaprStateMemory\n\n\n# Create an onboarding workflow agent\ndurable_agent = DurableAgent(\n    name=\"OnboardingWorkflowBot\",\n    role=\"Employee Onboarding Coordinator\",\n    instructions=[\n        \"Guide and automate multi-step onboarding processes\",\n        \"Track progress and retain state across sessions and failures\",\n        \"Coordinate with tools to provision accounts, access, and resources\",\n    ],\n    llm=OpenAIChatClient(),\n    tools=[\n        provision_email_account,\n        setup_github_access,\n        assign_kubernetes_namespace,\n        configure_slack_workspace,\n        request_hardware_kit,\n    ],\n    message_bus_name=\"messagepubsub\",\n    state_store_name=\"workflowstatestore\",\n    state_key=\"workflow_state\",\n    agents_registry_store_name=\"agentstatestore\",\n    agents_registry_key=\"agents_registry\",\n),\n</code></pre></p>"},{"location":"concepts/agents/#agent-patterns","title":"Agent Patterns","text":"<p>In Dapr Agents, Agent Patterns define the built-in loops that allow agents to dynamically handle tasks. These patterns enable agents to iteratively reason, act, and adapt, making them flexible and capable problem-solvers. By embedding these patterns, Dapr Agents ensures agents can independently complete tasks without requiring external orchestration.</p>"},{"location":"concepts/agents/#tool-calling","title":"Tool Calling","text":"<p>Tool Calling is an essential pattern in autonomous agent design, allowing AI agents to interact dynamically with external tools based on user input. One reliable method for enabling this is through OpenAI's Function Calling capability. This feature allows developers to describe functions to models trained to generate structured JSON objects containing the necessary arguments for tool execution, based on user queries.</p>"},{"location":"concepts/agents/#how-it-works","title":"How It Works","text":"<ol> <li>The user submits a query specifying a task and the available tools.</li> <li>The LLM analyzes the query and selects the right tool for the task.</li> <li>The LLM provides a structured JSON output containing the tool\u2019s unique ID, name, and arguments.</li> <li>The AI agent parses the JSON, executes the tool with the provided arguments, and sends the results back as a tool message.</li> <li>The LLM then summarizes the tool's execution results within the user\u2019s context to deliver a comprehensive final response.</li> </ol> <p>Info</p> <p>Steps 2-4 can be repeated multiple times, depending on the task's complexity.</p> <p>This pattern is highly flexible and supports multiple iterations of tool selection and execution, empowering agents to handle dynamic and multi-step tasks more effectively.</p>"},{"location":"concepts/agents/#react","title":"ReAct","text":"<p>The ReAct (Reason + Act) pattern was introduced in 2022 to enhance the capabilities of LLM-based AI agents by combining reasoning with action. This approach allows agents not only to reason through complex tasks but also to interact with the environment, taking actions based on their reasoning and observing the outcomes. ReAct enables AI agents to dynamically adapt to tasks by reasoning about the next steps and executing actions in real time.</p>"},{"location":"concepts/agents/#how-it-works_1","title":"How It Works","text":"<ul> <li>Thought (Reasoning): The agent analyzes the situation and generates a thought or a plan based on the input.</li> <li>Action: The agent takes an action based on its reasoning.</li> <li>Observation: After the action is executed, the agent observes the results or feedback from the environment, assessing the effectiveness of its action.</li> </ul> <p>Info</p> <p>These steps create a cycle that allows the agent to continuously think, act, and learn from the results.</p> <p>The ReAct pattern gives the agent the flexibility to adapt based on task complexity:</p> <ul> <li>Deep reasoning tasks: The agent goes through multiple cycles of thought, action, and observation to refine its responses.</li> <li>Action-driven tasks: The agent focuses more on timely actions and thinks critically only at key decision points.</li> </ul> <p>ReAct empowers agents to navigate complex, real-world environments efficiently, making them better suited for scenarios that require both deep reasoning and timely actions.</p>"},{"location":"concepts/agents/#agent-collaboration-using-workflows","title":"Agent Collaboration using Workflows","text":"<p>While patterns empower individual agents, workflows enable the coordination of multiple agents to achieve shared goals. In Dapr Agents, workflows serve as a higher-level framework for organizing how agents collaborate and distribute tasks.</p> <p>Workflows can orchestrate agents, each equipped with their own built-in patterns, to handle different parts of a larger process. For example, one agent might gather data using tools, another might analyze the results, and a third might generate a report. The workflow manages the communication and sequencing between these agents, ensuring smooth collaboration.</p> <p>Interestingly, workflows can also define loops similar to agent patterns. Instead of relying on an agent\u2019s built-in tool-calling loop, you can design workflows to orchestrate tool usage, reasoning, and action. This gives you the flexibility to use workflows to define both multi-agent collaboration and complex task handling for a single agent.</p>"},{"location":"concepts/agents/#random-workflow","title":"Random Workflow","text":"<p>In a Random Workflow, the next agent to handle a task is selected randomly. This approach:</p> <ul> <li>Encourages diversity in agent responses and strategies.</li> <li>Simplifies orchestration in cases where task assignment does not depend on specific agent roles or expertise.</li> <li>Random workflows are particularly useful for exploratory tasks or brainstorming scenarios where agent collaboration benefits from randomness.</li> </ul>"},{"location":"concepts/agents/#round-robin-workflow","title":"Round Robin Workflow","text":"<p>The Round Robin Workflow assigns tasks sequentially to agents in a fixed order. This method:</p> <ul> <li>Ensures equal participation among agents.</li> <li>Is ideal for scenarios requiring predictable task distribution, such as routine monitoring or repetitive processes.</li> <li>For example, in a team of monitoring agents, each agent takes turns analyzing incoming data streams in a predefined order.</li> </ul>"},{"location":"concepts/agents/#llm-based-workflow","title":"LLM-Based Workflow","text":"<p>The LLM-Based Workflow relies on the reasoning capabilities of an LLM to dynamically choose the next agent based on:</p> <ul> <li>Task Context: The nature and requirements of the current task.</li> <li>Chat History: Previous agent responses and interactions.</li> <li>Agent Metadata: Attributes like expertise, availability, or priorities.</li> </ul> <p>This approach ensures that the most suitable agent is selected for each task, optimizing collaboration and efficiency. For example, in a multi-agent customer support system, the LLM can assign tasks to agents based on customer issues, agent expertise, and workload distribution.</p>"},{"location":"concepts/arxiv_fetcher/","title":"Arxiv Fetcher","text":"<p>The Arxiv Fetcher module in <code>Dapr Agents</code> provides a powerful interface to interact with the arXiv API. It is designed to help users programmatically search for, retrieve, and download scientific papers from arXiv. With advanced querying capabilities, metadata extraction, and support for downloading PDF files, the Arxiv Fetcher is ideal for researchers, developers, and teams working with academic literature.</p>"},{"location":"concepts/arxiv_fetcher/#why-use-the-arxiv-fetcher","title":"Why Use the Arxiv Fetcher?","text":"<p>The Arxiv Fetcher simplifies the process of accessing research papers, offering features like:</p> <ul> <li>Automated Literature Search: Query arXiv for specific topics, keywords, or authors.</li> <li>Metadata Retrieval: Extract structured metadata, such as titles, abstracts, authors, categories, and submission dates.</li> <li>Precise Filtering: Limit search results by date ranges (e.g., retrieve the latest research in a field).</li> <li>PDF Downloading: Fetch full-text PDFs of papers for offline use.</li> </ul>"},{"location":"concepts/arxiv_fetcher/#how-to-use-the-arxiv-fetcher","title":"How to Use the Arxiv Fetcher","text":""},{"location":"concepts/arxiv_fetcher/#step-1-install-required-modules","title":"Step 1: Install Required Modules","text":"<p>Info</p> <p>The Arxiv Fetcher relies on a lightweight Python wrapper for the arXiv API, which is not included in the Dapr Agents core module. This design choice helps maintain modularity and avoids adding unnecessary dependencies for users who may not require this functionality. To use the Arxiv Fetcher, ensure you install the library separately.</p> <pre><code>pip install arxiv\n</code></pre>"},{"location":"concepts/arxiv_fetcher/#step-2-initialize-the-fetcher","title":"Step 2: Initialize the Fetcher","text":"<p>Set up the <code>ArxivFetcher</code> to begin interacting with the arXiv API.</p> <pre><code>from dapr_agents.document import ArxivFetcher\n\n# Initialize the fetcher\nfetcher = ArxivFetcher()\n</code></pre>"},{"location":"concepts/arxiv_fetcher/#step-3-perform-searches","title":"Step 3: Perform Searches","text":"<p>Basic Search by Query String</p> <p>Search for papers using simple keywords. The results are returned as Document objects, each containing:</p> <ul> <li><code>text</code>: The abstract of the paper.</li> <li><code>metadata</code>: Structured metadata such as title, authors, categories, and submission dates.</li> </ul> <pre><code># Search for papers related to \"machine learning\"\nresults = fetcher.search(query=\"machine learning\", max_results=5)\n\n# Display metadata and summaries\nfor doc in results:\n    print(f\"Title: {doc.metadata['title']}\")\n    print(f\"Authors: {', '.join(doc.metadata['authors'])}\")\n    print(f\"Summary: {doc.text}\\n\")\n</code></pre> <p>Advanced Querying</p> <p>Refine searches using logical operators like AND, OR, and NOT or perform field-specific searches, such as by author.</p> <p>Examples:</p> <p>Search for papers on \"agents\" and \"cybersecurity\":</p> <pre><code>results = fetcher.search(query=\"all:(agents AND cybersecurity)\", max_results=10)\n</code></pre> <p>Exclude specific terms (e.g., \"quantum\" but not \"computing\"):</p> <pre><code>results = fetcher.search(query=\"all:(quantum NOT computing)\", max_results=10)\n</code></pre> <p>Search for papers by a specific author:</p> <pre><code>results = fetcher.search(query='au:\"John Doe\"', max_results=10)\n</code></pre> <p>Filter Papers by Date</p> <p>Limit search results to a specific time range, such as papers submitted in the last 24 hours.</p> <pre><code>from datetime import datetime, timedelta\n\n# Calculate the date range\nlast_24_hours = (datetime.now() - timedelta(days=1)).strftime(\"%Y%m%d\")\ntoday = datetime.now().strftime(\"%Y%m%d\")\n\n# Search for recent papers\nrecent_results = fetcher.search(\n    query=\"all:(agents AND cybersecurity)\",\n    from_date=last_24_hours,\n    to_date=today,\n    max_results=5\n)\n\n# Display metadata\nfor doc in recent_results:\n    print(f\"Title: {doc.metadata['title']}\")\n    print(f\"Authors: {', '.join(doc.metadata['authors'])}\")\n    print(f\"Published: {doc.metadata['published']}\")\n    print(f\"Summary: {doc.text}\\n\")\n</code></pre>"},{"location":"concepts/arxiv_fetcher/#step-4-download-pdfs","title":"Step 4: Download PDFs","text":"<p>Fetch the full-text PDFs of papers for offline use. Metadata is preserved alongside the downloaded files.</p> <pre><code>import os\nfrom pathlib import Path\n\n# Create a directory for downloads\nos.makedirs(\"arxiv_papers\", exist_ok=True)\n\n# Download PDFs\ndownload_results = fetcher.search(\n    query=\"all:(agents AND cybersecurity)\",\n    max_results=5,\n    download=True,\n    dirpath=Path(\"arxiv_papers\")\n)\n\nfor paper in download_results:\n    print(f\"Downloaded Paper: {paper['title']}\")\n    print(f\"File Path: {paper['file_path']}\\n\")\n</code></pre>"},{"location":"concepts/arxiv_fetcher/#step-5-extract-and-process-pdf-content","title":"Step 5: Extract and Process PDF Content","text":"<p>Use <code>PyPDFReader</code> from <code>Dapr Agents</code> to extract content from downloaded PDFs. Each page is treated as a separate Document object with metadata.</p> <pre><code>from pathlib import Path\nfrom dapr_agents.document import PyPDFReader\n\nreader = PyPDFReader()\ndocs_read = []\n\nfor paper in download_results:\n    local_pdf_path = Path(paper[\"file_path\"])\n    documents = reader.load(local_pdf_path, additional_metadata=paper)\n    docs_read.extend(documents)\n\n# Verify results\nprint(f\"Extracted {len(docs_read)} documents.\")\nprint(f\"First document text: {docs_read[0].text}\")\nprint(f\"Metadata: {docs_read[0].metadata}\")\n</code></pre>"},{"location":"concepts/arxiv_fetcher/#practical-applications","title":"Practical Applications","text":"<p>The Arxiv Fetcher enables various use cases for researchers and developers:</p> <ul> <li>Literature Reviews: Quickly retrieve and organize relevant papers on a given topic or by a specific author.</li> <li>Trend Analysis: Identify the latest research in a domain by filtering for recent submissions.</li> <li>Offline Research Workflows: Download and process PDFs for local analysis and archiving.</li> </ul>"},{"location":"concepts/arxiv_fetcher/#next-steps","title":"Next Steps","text":"<p>While the Arxiv Fetcher provides robust functionality for retrieving and processing research papers, its output can be integrated into advanced workflows:</p> <ul> <li>Building a Searchable Knowledge Base: Combine fetched papers with tools like text splitting and vector embeddings for advanced search capabilities.</li> <li>Retrieval-Augmented Generation (RAG): Use processed papers as inputs for RAG pipelines to power question-answering systems.</li> <li>Automated Literature Surveys: Generate summaries or insights based on the fetched and processed research.</li> </ul>"},{"location":"concepts/messaging/","title":"Messaging","text":"<p>Messaging is how agents communicate, collaborate, and adapt in workflows. It enables them to share updates, execute tasks, and respond to events seamlessly. Messaging is one of the main components of <code>event-driven</code> agentic workflows, ensuring tasks remain scalable, adaptable, and decoupled. Built entirely around the <code>Pub/Sub (publish/subscribe)</code> model, messaging leverages a message bus to facilitate communication across agents, services, and workflows.</p>"},{"location":"concepts/messaging/#key-role-of-messaging-in-agentic-workflows","title":"Key Role of Messaging in Agentic Workflows","text":"<p>Messaging connects agents in workflows, enabling real-time communication and coordination. It acts as the backbone of event-driven interactions, ensuring that agents work together effectively without requiring direct connections.</p> <p>Through messaging, agents can:</p> <ul> <li>Collaborate Across Tasks: Agents exchange messages to share updates, broadcast events, or deliver task results.</li> <li>Orchestrate Workflows: Tasks are triggered and coordinated through published messages, enabling workflows to adjust dynamically.</li> <li>Respond to Events: Agents adapt to real-time changes by subscribing to relevant topics and processing events as they occur.</li> </ul> <p>By using messaging, workflows remain modular and scalable, with agents focusing on their specific roles while seamlessly participating in the broader system.</p>"},{"location":"concepts/messaging/#how-messaging-works","title":"How Messaging Works","text":"<p>Messaging relies on the <code>Pub/Sub</code> model, which organizes communication into topics. These topics act as channels where agents can publish and subscribe to messages, enabling efficient and decoupled communication.</p>"},{"location":"concepts/messaging/#message-bus-and-topics","title":"Message Bus and Topics","text":"<p>The message bus serves as the central system that manages topics and message delivery. Agents interact with the message bus to send and receive messages:</p> <ul> <li>Publishing Messages: Agents publish messages to a specific topic, making the information available to all subscribed agents.</li> <li>Subscribing to Topics: Agents subscribe to topics relevant to their roles, ensuring they only receive the messages they need.</li> <li>Broadcasting Updates: Multiple agents can subscribe to the same topic, allowing them to act on shared events or updates.</li> </ul>"},{"location":"concepts/messaging/#scalability-and-adaptability","title":"Scalability and Adaptability","text":"<p>The message bus ensures that communication scales effortlessly, whether you are adding new agents, expanding workflows, or adapting to changing requirements. Agents remain loosely coupled, allowing workflows to evolve without disruptions.</p>"},{"location":"concepts/messaging/#messaging-in-event-driven-workflows","title":"Messaging in Event-Driven Workflows","text":"<p>Event-driven workflows depend on messaging to enable dynamic and real-time interactions. Unlike deterministic workflows, which follow a fixed sequence of tasks, event-driven workflows respond to the messages and events flowing through the system.</p> <ul> <li>Real-Time Triggers: Agents can initiate tasks or workflows by publishing specific events.</li> <li>Asynchronous Execution: Tasks are coordinated through messages, allowing agents to operate independently and in parallel.</li> <li>Dynamic Adaptation: Agents adjust their behavior based on the messages they receive, ensuring workflows remain flexible and resilient.</li> </ul>"},{"location":"concepts/messaging/#why-pubsub-messaging-for-agentic-workflows","title":"Why Pub/Sub Messaging for Agentic Workflows?","text":"<p>Pub/Sub messaging is essential for event-driven agentic workflows because it:</p> <ul> <li>Decouples Components: Agents publish messages without needing to know which agents will receive them, promoting modular and scalable designs.</li> <li>Enables Real-Time Communication: Messages are delivered as events occur, allowing agents to react instantly.</li> <li>Fosters Collaboration: Multiple agents can subscribe to the same topic, making it easy to share updates or divide responsibilities.</li> </ul> <p>This messaging framework ensures that agents operate efficiently, workflows remain flexible, and systems can scale dynamically.</p>"},{"location":"concepts/messaging/#conclusion","title":"Conclusion","text":"<p>Messaging is the backbone of event-driven agentic workflows. By leveraging a robust Pub/Sub model, agents communicate efficiently, adapt dynamically, and collaborate seamlessly. This foundation ensures that workflows scale, evolve, and respond in real time, empowering agents to achieve their goals in a shared, dynamic environment.</p>"},{"location":"concepts/text_splitter/","title":"Text Splitter","text":"<p>The Text Splitter module is a foundational tool in <code>Dapr Agents</code> designed to preprocess documents for use in Retrieval-Augmented Generation (RAG) workflows and other <code>in-context learning</code> applications. Its primary purpose is to break large documents into smaller, meaningful chunks that can be embedded, indexed, and efficiently retrieved based on user queries.</p> <p>By focusing on manageable chunk sizes and preserving contextual integrity through overlaps, the Text Splitter ensures documents are processed in a way that supports downstream tasks like question answering, summarization, and document retrieval.</p>"},{"location":"concepts/text_splitter/#why-use-a-text-splitter","title":"Why Use a Text Splitter?","text":"<p>When building RAG pipelines, splitting text into smaller chunks serves these key purposes:</p> <ul> <li>Enabling Effective Indexing: Chunks are embedded and stored in a vector database, making them retrievable based on similarity to user queries.</li> <li>Maintaining Semantic Coherence: Overlapping chunks help retain context across splits, ensuring the system can connect related pieces of information.</li> <li>Handling Model Limitations: Many models have input size limits. Splitting ensures text fits within these constraints while remaining meaningful.</li> </ul> <p>This step is crucial for preparing knowledge to be embedded into a searchable format, forming the backbone of retrieval-based workflows.</p>"},{"location":"concepts/text_splitter/#strategies-for-text-splitting","title":"Strategies for Text Splitting","text":"<p>The Text Splitter supports multiple strategies to handle different types of documents effectively. These strategies balance the size of each chunk with the need to maintain context.</p>"},{"location":"concepts/text_splitter/#1-character-based-length","title":"1. Character-Based Length","text":"<ul> <li>How It Works: Counts the number of characters in each chunk.</li> <li>Use Case: Simple and effective for text splitting without dependency on external tokenization tools.</li> </ul> <p>Example:</p> <pre><code>from dapr_agents.document.splitter.text import TextSplitter\n\n# Character-based splitter (default)\nsplitter = TextSplitter(chunk_size=1024, chunk_overlap=200)\n</code></pre>"},{"location":"concepts/text_splitter/#2-token-based-length","title":"2. Token-Based Length","text":"<ul> <li>How It Works: Counts tokens, which are the semantic units used by language models (e.g., words or subwords).</li> <li>Use Case: Ensures compatibility with models like GPT, where token limits are critical.</li> </ul> <p>Example:</p> <pre><code>import tiktoken\nfrom dapr_agents.document.splitter.text import TextSplitter\n\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\ndef length_function(text: str) -&gt; int:\n    return len(enc.encode(text))\n\nsplitter = TextSplitter(\n    chunk_size=1024,\n    chunk_overlap=200,\n    chunk_size_function=length_function\n)\n</code></pre> <p>The flexibility to define the chunk size function makes the Text Splitter adaptable to various scenarios.</p>"},{"location":"concepts/text_splitter/#chunk-overlap","title":"Chunk Overlap","text":"<p>To preserve context, the Text Splitter includes a chunk overlap feature. This ensures that parts of one chunk carry over into the next, helping maintain continuity when chunks are processed sequentially.</p> <p>Example:</p> <ul> <li>With <code>chunk_size=1024</code> and <code>chunk_overlap=200</code>, the last <code>200</code> tokens or characters of one chunk appear at the start of the next.</li> <li>This design helps in tasks like text generation, where maintaining context across chunks is essential.</li> </ul>"},{"location":"concepts/text_splitter/#how-to-use-the-text-splitter","title":"How to Use the Text Splitter","text":"<p>Here\u2019s a practical example of using the Text Splitter to process a PDF document:</p>"},{"location":"concepts/text_splitter/#step-1-load-a-pdf","title":"Step 1: Load a PDF","text":"<pre><code>import requests\nfrom pathlib import Path\n\n# Download PDF\npdf_url = \"https://arxiv.org/pdf/2412.05265.pdf\"\nlocal_pdf_path = Path(\"arxiv_paper.pdf\")\n\nif not local_pdf_path.exists():\n    response = requests.get(pdf_url)\n    response.raise_for_status()\n    with open(local_pdf_path, \"wb\") as pdf_file:\n        pdf_file.write(response.content)\n</code></pre>"},{"location":"concepts/text_splitter/#step-2-read-the-document","title":"Step 2: Read the Document","text":"<p>For this example, we use Dapr Agents' <code>PyPDFReader</code>.</p> <p>Info</p> <p>The PyPDF Reader relies on the pypdf python library, which is not included in the Dapr Agents core module. This design choice helps maintain modularity and avoids adding unnecessary dependencies for users who may not require this functionality. To use the PyPDF Reader, ensure that you install the library separately.</p> <pre><code>pip install pypdf\n</code></pre> <p>Then, initialize the reader to load the PDF file.</p> <pre><code>from dapr_agents.document.reader.pdf.pypdf import PyPDFReader\n\nreader = PyPDFReader()\ndocuments = reader.load(local_pdf_path)\n</code></pre>"},{"location":"concepts/text_splitter/#step-3-split-the-document","title":"Step 3: Split the Document","text":"<pre><code>splitter = TextSplitter(\n    chunk_size=1024,\n    chunk_overlap=200,\n    chunk_size_function=length_function\n)\nchunked_documents = splitter.split_documents(documents)\n</code></pre>"},{"location":"concepts/text_splitter/#step-4-analyze-results","title":"Step 4: Analyze Results","text":"<pre><code>print(f\"Original document pages: {len(documents)}\")\nprint(f\"Total chunks: {len(chunked_documents)}\")\nprint(f\"First chunk: {chunked_documents[0]}\")\n</code></pre>"},{"location":"concepts/text_splitter/#key-features","title":"Key Features","text":"<ul> <li>Hierarchical Splitting: Splits text by separators (e.g., paragraphs), then refines chunks further if needed.</li> <li>Customizable Chunk Size: Supports character-based and token-based length functions.</li> <li>Overlap for Context: Retains portions of one chunk in the next to maintain continuity.</li> <li>Metadata Preservation: Each chunk retains metadata like page numbers and start/end indices for easier mapping.</li> </ul> <p>By understanding and leveraging the <code>Text Splitter</code>, you can preprocess large documents effectively, ensuring they are ready for embedding, indexing, and retrieval in advanced workflows like RAG pipelines.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#dependencies","title":"Dependencies","text":"<p>This project uses modern Python packaging with <code>pyproject.toml</code>. Dependencies are managed as follows:</p> <ul> <li>Main dependencies are in <code>[project.dependencies]</code></li> <li>Test dependencies are in <code>[project.optional-dependencies.test]</code></li> <li>Development dependencies are in <code>[project.optional-dependencies.dev]</code></li> </ul>"},{"location":"development/#generating-requirements-files","title":"Generating Requirements Files","text":"<p>If you need to generate requirements files (e.g., for deployment or specific environments):</p> <pre><code># Generate requirements.txt\npip-compile pyproject.toml\n\n# Generate dev-requirements.txt\npip-compile pyproject.toml --extra dev\n</code></pre>"},{"location":"development/#installing-dependencies","title":"Installing Dependencies","text":"<pre><code># Install main package with test dependencies\npip install -e \".[test]\"\n\n# Install main package with development dependencies\npip install -e \".[dev]\"\n\n# Install main package with all optional dependencies\npip install -e \".[test,dev]\"\n</code></pre>"},{"location":"development/#testing","title":"Testing","text":"<p>The project uses pytest for testing. To run tests:</p> <pre><code># Run all tests\ntox -e pytest\n\n# Run specific test file\ntox -e pytest tests/test_random_orchestrator.py\n\n# Run tests with coverage\ntox -e pytest --cov=dapr_agents\n</code></pre>"},{"location":"development/#code-quality","title":"Code Quality","text":"<p>The project uses several tools to maintain code quality:</p> <pre><code># Run linting\ntox -e flake8\n\n# Run code formatting\ntox -e ruff\n\n# Run type checking\ntox -e type\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Run tests before making changes:    <pre><code>tox -e pytest\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li> <p>Run code quality checks:    <pre><code>tox -e flake8\ntox -e ruff\ntox -e type\n</code></pre></p> </li> <li> <p>Run tests again:    <pre><code>tox -e pytest\n</code></pre></p> </li> <li> <p>Submit your changes</p> </li> </ol>"},{"location":"home/installation/","title":"Installation","text":""},{"location":"home/installation/#install-dapr-agents","title":"Install Dapr Agents","text":"<p>Info</p> <p>make sure you have Python already installed. <code>Python &gt;=3.10</code></p>"},{"location":"home/installation/#as-a-python-package-using-pip","title":"As a Python package using Pip","text":"<pre><code>pip install dapr-agents\n</code></pre>"},{"location":"home/installation/#install-dapr-cli","title":"Install Dapr CLI","text":"<p>Install the Dapr CLI to manage Dapr-related tasks like running applications with sidecars, viewing logs, and launching the Dapr dashboard. It works seamlessly with both self-hosted and Kubernetes environments. For a complete step-by-step guide, visit the official Dapr CLI installation page.</p> <p>Verify the CLI is installed by restarting your terminal/command prompt and running the following:</p> <pre><code>dapr -h\n</code></pre>"},{"location":"home/installation/#initialize-dapr-in-local-mode","title":"Initialize Dapr in Local Mode","text":"<p>Info</p> <p>Make sure you have Docker already installed. I use Docker Desktop.</p> <p>Initialize Dapr locally to set up a self-hosted environment for development. This process fetches and installs the Dapr sidecar binaries, runs essential services as Docker containers, and prepares a default components folder for your application. For detailed steps, see the official guide on initializing Dapr locally.</p> <p></p> <p>To initialize the Dapr control plane containers and create a default configuration file, run:</p> <pre><code>dapr init\n</code></pre> <p>Verify you have container instances with <code>daprio/dapr</code>, <code>openzipkin/zipkin</code>, and <code>redis</code> images running:</p> <pre><code>docker ps\n</code></pre>"},{"location":"home/installation/#enable-redis-insights","title":"Enable Redis Insights","text":"<p>Dapr uses Redis by default for state management and pub/sub messaging, which are fundamental to Dapr Agents's agentic workflows. These capabilities enable the following:</p> <ul> <li>Viewing Pub/Sub Messages: Monitor and inspect messages exchanged between agents in event-driven workflows.</li> <li>Inspecting State Information: Access and analyze shared state data among agents.</li> <li>Debugging and Monitoring Events: Track workflow events in real time to ensure smooth operations and identify issues.</li> </ul> <p>To make these insights more accessible, you can leverage Redis Insight.</p> <pre><code>docker run --rm -d --name redisinsight -p 5540:5540 redis/redisinsight:latest\n</code></pre> <p>Once running, access the Redis Insight interface at <code>http://localhost:5540/</code></p>"},{"location":"home/installation/#connection-configuration","title":"Connection Configuration","text":"<ul> <li>Port: 6379</li> <li>Host (Linux): 172.17.0.1</li> <li>Host (Windows/Mac): host.docker.internal (example <code>host.docker.internal:6379</code>)</li> </ul> <p>Redis Insight makes it easy to visualize and manage the data powering your agentic workflows, ensuring efficient debugging, monitoring, and optimization.</p> <p></p>"},{"location":"home/installation/#using-custom-endpoints","title":"Using custom endpoints","text":""},{"location":"home/installation/#azure-hosted-openai-endpoint","title":"Azure hosted OpenAI endpoint","text":"<p>In order to use Azure OpenAI for the model you'll need the following <code>.env</code> file:</p> <pre><code>AZURE_OPENAI_API_KEY=your_custom_key\nAZURE_OPENAI_ENDPOINT=your_custom_endpoint\nAZURE_OPENAI_DEPLOYMENT=your_custom_model\nAZURE_OPENAI_API_VERSION=\"azure_openai_api_version\"\n</code></pre> <p>NB! the <code>AZURE_OPENAI_DEPLOYMENT</code> refers to the model, e.g., <code>gpt-4o</code>. <code>AZURE_OPENAI_API_VERSION</code> has been tested to work against <code>2024-08-01-preview</code>.</p> <p>Then instantiate the agent(s) as well as the orchestrator as follows:</p> <pre><code>from dapr_agents import DurableAgent, OpenAIChatClient\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\nimport os\n\nasync def main():\n    llm = OpenAIChatClient(\n        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n        azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n    )\n\n    try:\n        elf_service = DurableAgent(\n            name=\"Legolas\", role=\"Elf\",\n            goal=\"Act as a scout, marksman, and protector, using keen senses and deadly accuracy to ensure the success of the journey.\",\n            instructions=[\n                \"Speak like Legolas, with grace, wisdom, and keen observation.\",\n                \"Be swift, silent, and precise, moving effortlessly across any terrain.\",\n                \"Use superior vision and heightened senses to scout ahead and detect threats.\",\n                \"Excel in ranged combat, delivering pinpoint arrow strikes from great distances.\",\n                \"Respond concisely, accurately, and relevantly, ensuring clarity and strict alignment with the task.\"],\n            llm=llm, # Note the explicit reference to the above OpenAIChatClient \n            message_bus_name=\"messagepubsub\",\n            state_store_name=\"workflowstatestore\",\n            state_key=\"workflow_state\",\n            agents_registry_store_name=\"agentstatestore\",\n            agents_registry_key=\"agents_registry\",\n        )\n...\n</code></pre> <p>The above is taken from multi-agent quick starter.</p>"},{"location":"home/principles/","title":"Core Principles","text":""},{"location":"home/principles/#1-agent-centric-design","title":"1. Agent-Centric Design","text":"<p>Dapr Agents is designed to place agents, powered by LLMs, at the core of task execution and workflow orchestration. This principle emphasizes:</p> <ul> <li>LLM-Powered Agents: Dapr Agents enables the creation of agents that leverage LLMs for reasoning, dynamic decision-making, and natural language interactions.</li> <li>Adaptive Task Handling: Agents in Dapr Agents are equipped with flexible patterns like tool calling and reasoning loops (e.g., ReAct), allowing them to autonomously tackle complex and evolving tasks.</li> <li>Seamless Integration: Dapr Agents\u2019 framework allows agents to act as modular, reusable building blocks that integrate seamlessly into workflows, whether they operate independently or collaboratively.</li> </ul> <p>While Dapr Agents centers around agents, it also recognizes the versatility of using LLMs directly in deterministic workflows or simpler task sequences. In scenarios where the agent's built-in task-handling patterns, like <code>tool calling</code> or <code>ReAct</code> loops, are unnecessary, LLMs can act as core components for reasoning and decision-making. This flexibility ensures users can adapt Dapr Agents to suit diverse needs without being confined to a single approach.</p> <p>Info</p> <p>Agents are not standalone; they are building blocks in larger, orchestrated workflows.</p>"},{"location":"home/principles/#2-decoupled-infrastructure-design","title":"2. Decoupled Infrastructure Design","text":"<p>Dapr Agents ensures a clean separation between agents and the underlying infrastructure, emphasizing simplicity, scalability, and adaptability:</p> <ul> <li>Agent Simplicity: Agents focus purely on reasoning and task execution, while Pub/Sub messaging, routing, and validation are managed externally by modular infrastructure components.</li> <li>Scalable and Adaptable Systems: By offloading non-agent-specific responsibilities, Dapr Agents allows agents to scale independently and adapt seamlessly to new use cases or integrations.</li> </ul> <p>Info</p> <p>Decoupling infrastructure keeps agents focused on tasks while enabling seamless scalability and integration across systems.</p> <p></p>"},{"location":"home/principles/#3-modular-component-model","title":"3. Modular Component Model","text":"<p>Dapr Agents utilizes Dapr's pluggable component framework and building blocks to simplify development and enhance flexibility:</p> <ul> <li>Building Blocks for Core Functionality: Dapr provides API building blocks, such as Pub/Sub messaging, state management, service invocation, and more, to address common microservice challenges and promote best practices.</li> <li>Interchangeable Components: Each building block operates on swappable components (e.g., Redis, Kafka, Azure CosmosDB), allowing you to replace implementations without changing application code.</li> <li>Seamless Transitions: Develop locally with default configurations and deploy effortlessly to cloud environments by simply updating component definitions.</li> <li>Scalable Foundations: Build resilient and adaptable architectures using Dapr\u2019s modular, production-ready building blocks.</li> </ul> <p>Info</p> <p>Developers can easily switch between different components (e.g., Redis to DynamoDB) based on their deployment environment, ensuring portability and adaptability.</p> <p></p>"},{"location":"home/principles/#4-actor-based-model-for-agents","title":"4. Actor-Based Model for Agents","text":"<p>Dapr Agents leverages Dapr\u2019s Virtual Actor model to enable agents to function efficiently and flexibly within distributed environments. Each agent in Dapr Agents is instantiated as an instance of a class, wrapped and managed by a virtual actor. This design offers:</p> <ul> <li>Stateful Agents: Virtual actors allow agents to store and recall information across tasks, maintaining context and continuity for workflows.</li> <li>Dynamic Lifecycle Management: Virtual actors are automatically instantiated when invoked and deactivated when idle. This eliminates the need for explicit creation or cleanup, ensuring resource efficiency and simplicity.</li> <li>Location Transparency: Agents can be accessed and operate seamlessly, regardless of where they are located in the system. The underlying runtime handles their mobility, enabling fault-tolerance and dynamic load balancing.</li> <li>Scalable Execution: Agents process one task at a time, avoiding concurrency issues, and scale dynamically across nodes to meet workload demands.</li> </ul> <p>This model ensures agents remain focused on their core logic, while the infrastructure abstracts complexities like state management, fault recovery, and resource optimization.</p> <p>Info</p> <p>Dapr Agents\u2019 use of virtual actors makes agents always addressable and highly scalable, enabling them to operate reliably and efficiently in distributed, high-demand environments.</p>"},{"location":"home/principles/#5-message-driven-communication","title":"5. Message-Driven Communication","text":"<p>Dapr Agents emphasizes the use of Pub/Sub messaging for event-driven communication between agents. This principle ensures:</p> <ul> <li>Decoupled Architecture: Asynchronous communication for scalability and modularity.</li> <li>Real-Time Adaptability: Agents react dynamically to events for faster, more flexible task execution.</li> <li>Seamless Collaboration: Agents share updates, distribute tasks, and respond to events in a highly coordinated way.</li> </ul> <p>Info</p> <p>Pub/Sub messaging serves as the backbone for Dapr Agents\u2019 event-driven workflows, enabling agents to communicate and collaborate in real time.</p> <p></p>"},{"location":"home/principles/#6-workflow-oriented-design","title":"6. Workflow-Oriented Design","text":"<p>Dapr Agents embraces workflows as a foundational concept, integrating Dapr Workflows to support both deterministic and event-driven task orchestration. This dual approach enables robust and adaptive systems:</p> <ul> <li>Deterministic Workflows: Dapr Agents uses Dapr Workflows for stateful, predictable task sequences. These workflows ensure reliable execution, fault tolerance, and state persistence, making them ideal for structured, multi-step processes that require clear, repeatable logic.</li> <li>Event-Driven Workflows: By combining Dapr Workflows with Pub/Sub messaging, Dapr Agents supports workflows that adapt to real-time events. This facilitates decentralized, asynchronous collaboration between agents, allowing workflows to dynamically adjust to changing scenarios.</li> </ul> <p>By integrating these paradigms, Dapr Agents enables workflows that combine the reliability of deterministic execution with the adaptability of event-driven processes, ensuring flexibility and resilience in a wide range of applications.</p> <p>Info</p> <p>Dapr Agents workflows blend structured, predictable logic with the dynamic responsiveness of event-driven systems, empowering both centralized and decentralized workflows.</p> <p></p>"},{"location":"home/why/","title":"Why Dapr Agents","text":"<p>Dapr Agents is an open-source framework for building and orchestrating LLM-based autonomous agents, designed to simplify the complexity of creating scalable agentic workflows and microservices. Inspired by the growing need for frameworks that integrate seamlessly with distributed systems, Dapr Agents enables developers to focus on designing intelligent agents without getting bogged down by infrastructure concerns.</p>"},{"location":"home/why/#the-problem","title":"The Problem","text":"<p>Many agentic frameworks today attempt to redefine how microservices are built and orchestrated by developing their own platforms for workflows, Pub/Sub messaging, state management, and service communication. While these efforts showcase innovation, they often lead to a steep learning curve, fragmented systems, and unnecessary complexity when scaling or adapting to new environments.</p> <p>Many of these frameworks require developers to adopt entirely new paradigms or recreate foundational infrastructure, rather than building on existing solutions that are proven to handle these challenges at scale. This added complexity often diverts focus from the primary goal: designing and implementing intelligent, effective agents.</p>"},{"location":"home/why/#dapr-agents-approach","title":"Dapr Agents' Approach","text":"<p>Dapr Agents takes a distinct approach by building on Dapr, a portable and event-driven runtime optimized for distributed systems. Dapr offers built-in APIs and patterns such as state management, Pub/Sub messaging, service invocation, and virtual actors\u2014that eliminate the need to recreate foundational components from scratch. By integrating seamlessly with Dapr, Dapr Agents empowers developers to focus on the intelligence and behavior of LLM-powered agents while leveraging a proven framework for scalability and reliability.</p> <p>Rather than reinventing microservices, Dapr Agents enables developers to design, test, and deploy agents that seamlessly integrate as collaborative services within larger systems. Whether experimenting with a single agent or orchestrating workflows involving multiple agents, Dapr Agents simplifies the exploration and implementation of scalable agentic workflows.</p>"},{"location":"home/why/#conclusion","title":"Conclusion","text":"<p>Dapr Agents provides a unified framework for designing, deploying, and orchestrating LLM-powered agents. By leveraging Dapr\u2019s runtime and modular components, Dapr Agents allows developers to focus on building intelligent systems without worrying about the complexities of distributed infrastructure. Whether you're creating standalone agents or orchestrating multi-agent workflows, Dapr Agents empowers you to explore the future of intelligent, scalable, and collaborative systems.</p>"},{"location":"home/quickstarts/","title":"Dapr Agents Quickstarts","text":"<p>Quickstarts demonstrate how to use Dapr Agents to build applications with LLM-powered autonomous agents and event-driven workflows. Each quickstart builds upon the previous one, introducing new concepts incrementally.</p> <p>Info</p> <p>Not all quickstarts require Docker, but it is recommended to have your local Dapr environment set up with Docker for the best development experience and to follow the steps in this guide seamlessly.</p>"},{"location":"home/quickstarts/#quickstarts","title":"Quickstarts","text":"Scenario What You'll Learn Hello WorldA rapid introduction that demonstrates core Dapr Agents concepts through simple, practical examples. - Basic LLM Usage: Simple text generation with OpenAI models  - Creating Agents: Building agents with custom tools in under 20 lines of code  - ReAct Pattern: Implementing reasoning and action cycles in agents  - Simple Workflows: Setting up multi-step LLM processes LLM Call with Dapr Chat ClientExplore interaction with Language Models through Dapr Agents' <code>DaprChatClient</code>, featuring basic text generation with plain text prompts and templates. - Text Completion: Generating responses to prompts  - Swapping LLM providers: Switching LLM backends without application code change  - Resilience: Setting timeout, retry and circuit-breaking  - PII Obfuscation: Automatically detect and mask sensitive user information LLM Call with OpenAI ClientDiscover how to leverage native LLM client libraries with Dapr Agents using the OpenAI Client for chat completion, audio processing, and embeddings. - Text Completion: Generating responses to prompts  - Structured Outputs: Converting LLM responses to Pydantic objects  Note: Other quickstarts for specific clients are available for Elevenlabs, Hugging Face, and Nvidia. Agent Tool CallBuild your first AI agent with custom tools by creating a practical weather assistant that fetches information and performs actions. - Tool Definition: Creating reusable tools with the <code>@tool</code> decorator  - Agent Configuration: Setting up agents with roles, goals, and tools  - Function Calling: Enabling LLMs to execute Python functions Agentic WorkflowDive into stateful workflows with Dapr Agents by orchestrating sequential and parallel tasks through powerful workflow capabilities. - LLM-powered Tasks: Using language models in workflows  - Task Chaining: Creating resilient multi-step processes executing in sequence  - Fan-out/Fan-in: Executing activities in parallel; then synchronizing these activities until all preceding activities have completed Multi-Agent WorkflowsExplore advanced event-driven workflows featuring a Lord of the Rings themed multi-agent system where autonomous agents collaborate to solve problems. - Multi-agent Systems: Creating a network of specialized agents  - Event-driven Architecture: Implementing pub/sub messaging between agents  - Actor Model: Using Dapr Actors for stateful agent management  - Workflow Orchestration: Coordinating agents through different selection strategies  Note: To see Actor-based workflow see Multi-Agent Actors. Multi-Agent Workflow on KubernetesRun multi-agent workflows in Kubernetes, demonstrating deployment and orchestration of event-driven agent systems in a containerized environment. - Kubernetes Deployment: Running agents on Kubernetes  - Container Orchestration: Managing agent lifecycles with K8s  - Service Communication: Inter-agent communication in K8s Document Agent with ChainlitCreate a conversational agent with an operational UI that can upload, and learn unstructured documents while retaining long-term memory. - Conversational Document Agent: Upload and converse over unstructured documents  - Cloud Agnostic Storage: Upload files to multiple storage providers  - Conversation Memory Storage: Persists conversation history using external storage. Data Agent with MCP and ChainlitBuild a conversational agent over a Postgres database using Model Composition Protocol (MCP) with a ChatGPT-like interface. - Database Querying: Natural language queries to relational databases  - MCP Integration: Connecting to databases without DB-specific code  - Data Analysis: Complex data analysis through conversation"},{"location":"home/quickstarts/agentic_workflows/","title":"Agentic Workflows","text":"<p>Info</p> <p>This quickstart requires <code>Dapr CLI</code> and <code>Docker</code>. You must have your local Dapr environment set up.</p> <p>Traditional workflows follow fixed, step-by-step processes, while autonomous agents make real-time decisions based on reasoning and available data. Agentic workflows combine the best of both approaches, integrating structured execution with reasoning loops to enable more adaptive decision-making.</p> <p>This allows systems to analyze information, adjust to new conditions, and refine actions dynamically rather than strictly following a predefined sequence. By incorporating planning, feedback loops, and model-driven adjustments, agentic workflows provide both scalability and predictability while still allowing for autonomous adaptation.</p> <p>In <code>Dapr Agents</code>, agentic workflows leverage LLM-based tasks, reasoning loop patterns, and an event-driven system powered by pub/sub messaging and a shared message bus. Agents operate autonomously, responding to events in real time, making decisions, and collaborating dynamically. This makes the system highly adaptable\u2014agents can communicate, share tasks, and adjust based on new information, ensuring fluid coordination across distributed environments. This approach is particularly useful for decentralized systems that require flexible, intelligent collaboration across multiple agents and applications.</p> <p>Tip</p> <p>We will demonstrate this concept using the Multi-Agent Workflow Guide from our Cookbook, which outlines a step-by-step guide to implementing a basic agentic workflow.</p>"},{"location":"home/quickstarts/agentic_workflows/#agents-as-services-dapr-workflows","title":"Agents as Services: Dapr Workflows","text":"<p>In <code>Dapr Agents</code>, agents can be implemented using Dapr Workflows, both of which are exposed as microservices via FastAPI servers.</p>"},{"location":"home/quickstarts/agentic_workflows/#agents-as-dapr-workflows-orchestration-complex-execution","title":"Agents as Dapr Workflows (Orchestration, Complex Execution)","text":"<p>Dapr Workflows define the structured execution of agent behaviors, reasoning loops, and tool selection. Workflows allow agents to:</p> <p>\u2705 Define complex execution sequences instead of just reacting to events. \u2705 Integrate with message buses to listen and act on real-time inputs. \u2705 Orchestrate multi-step reasoning, retrieval-augmented generation (RAG), and tool use. \u2705 Best suited for goal-driven, structured, and iterative decision-making workflows.</p> <p>\ud83d\ude80 Dapr agents uses Dapr Workflows for orchestration and complex multi-agent collaboration.</p> <p>Example: An Agent as a Dapr Workflow</p> <pre><code>from dapr_agents import DurableAgent\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        # Define Agent\n        wizard_service = DurableAgent(\n            name=\"Gandalf\",\n            role=\"Wizard\",\n            goal=\"Guide the Fellowship with wisdom and strategy, using magic and insight to ensure the downfall of Sauron.\",\n            instructions=[\n                \"Speak like Gandalf, with wisdom, patience, and a touch of mystery.\",\n                \"Provide strategic counsel, always considering the long-term consequences of actions.\",\n                \"Use magic sparingly, applying it when necessary to guide or protect.\",\n                \"Encourage allies to find strength within themselves rather than relying solely on your power.\",\n                \"Respond concisely, accurately, and relevantly, ensuring clarity and strict alignment with the task.\"\n            ],\n            message_bus_name=\"messagepubsub\",\n            state_store_name=\"agenticworkflowstate\",\n            state_key=\"workflow_state\",\n            agents_registry_store_name=\"agentsregistrystore\",\n            agents_registry_key=\"agents_registry\",\n        )\n\n        await wizard_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Here, <code>Gandalf</code> is an <code>DurableAgent</code> implemented as a workflow, meaning it executes structured reasoning, plans actions, and integrates tools within a managed workflow execution loop.</p>"},{"location":"home/quickstarts/agentic_workflows/#how-we-use-dapr-workflows-for-orchestration","title":"How We Use Dapr Workflows for Orchestration","text":"<p>In dapr agents, the orchestrator itself is a Dapr Workflow, which:</p> <p>\u2705 Coordinates execution of agentic workflows (LLM-driven or rule-based). \u2705 Delegates tasks to agents implemented as either other workflows. \u2705 Manages reasoning loops, plan adaptation, and error handling dynamically.</p> <p>\ud83d\ude80 The LLM default orchestrator is a Dapr Workflow that interacts with agent workflows.</p> <p>Example: The Orchestrator as a Dapr Workflow</p> <pre><code>from dapr_agents import LLMOrchestrator\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        agentic_orchestrator = LLMOrchestrator(\n            name=\"Orchestrator\",\n            message_bus_name=\"messagepubsub\",\n            state_store_name=\"agenticworkflowstate\",\n            state_key=\"workflow_state\",\n            agents_registry_store_name=\"agentsregistrystore\",\n            agents_registry_key=\"agents_registry\",\n            max_iterations=25\n        ).as_service(port=8009)\n\n        await agentic_orchestrator.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>This orchestrator acts as a central controller, ensuring that agentic workflows communicate effectively, execute tasks in order, and handle iterative reasoning loops.</p>"},{"location":"home/quickstarts/agentic_workflows/#structuring-a-multi-agent-project","title":"Structuring A Multi-Agent Project","text":"<p>The way to structure such a project is straightforward. We organize our services into a directory that contains individual folders for each agent, along with a <code>components</code> directory for Dapr resources configurations. Each agent service includes its own app.py file, where the FastAPI server and the agent logic are defined.</p> <pre><code>dapr.yaml                  # Dapr main config file\ncomponents/                # Dapr resource files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#set-up-an-environment-variables-file","title":"Set Up an Environment Variables File","text":"<p>This example uses our default <code>LLM Orchestrator</code>. Therefore, you have to create an <code>.env</code> file to securely store your Inference Service (i.e. OpenAI) API keys and other sensitive information. For example:</p> <pre><code>OPENAI_API_KEY=\"your-api-key\"\nOPENAI_BASE_URL=\"https://api.openai.com/v1\"\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#define-your-first-agent-service","title":"Define Your First Agent Service","text":"<p>Let's start by definining a <code>Hobbit</code> service with a specific <code>name</code>, <code>role</code>, <code>goal</code> and <code>instructions</code>.</p> <pre><code>services/                  # Directory for agent services\n\u251c\u2500\u2500 hobbit/                # Hobbit Service\n\u2502   \u251c\u2500\u2500 app.py             # Dapr Enabled FastAPI app for Hobbit\n</code></pre> <p>Create the <code>app.py</code> script and provide the following information.</p> <pre><code>from dapr_agents import DurableAgent\nfrom dotenv import load_dotenv\nimport asyncio\nimport logging\n\nasync def main():\n    try:\n        # Define Agent and expose it as a service\n        hobbit_agent = DurableAgent(\n            role=\"Hobbit\",\n            name=\"Frodo\",\n            goal=\"Carry the One Ring to Mount Doom, resisting its corruptive power while navigating danger and uncertainty.\",\n            instructions=[\n                \"Speak like Frodo, with humility, determination, and a growing sense of resolve.\",\n                \"Endure hardships and temptations, staying true to the mission even when faced with doubt.\",\n                \"Seek guidance and trust allies, but bear the ultimate burden alone when necessary.\",\n                \"Move carefully through enemy-infested lands, avoiding unnecessary risks.\",\n                \"Respond concisely, accurately, and relevantly, ensuring clarity and strict alignment with the task.\"\n            ],\n            message_bus_name=\"messagepubsub\",\n            agents_registry_store_name=\"agentsregistrystore\",\n            agents_registry_key=\"agents_registry\",\n        ).as_service(8001_)\n\n        await hobbit_service.start()\n    except Exception as e:\n        print(f\"Error starting service: {e}\")\n\nif __name__ == \"__main__\":\n    load_dotenv()\n\n    logging.basicConfig(level=logging.INFO)\n\n    asyncio.run(main())\n</code></pre> <p>Now, you can define multiple services following this format, but it's essential to pay attention to key areas to ensure everything runs smoothly. Specifically, focus on correctly configuring the components (e.g., <code>statestore</code> and <code>pubsub</code> names) and incrementing the ports for each service.</p> <p>Key Considerations:</p> <ul> <li>Ensure the <code>message_bus_name</code> matches the <code>pub/sub</code> component name in your <code>pubsub.yaml</code> file.</li> <li>Verify the <code>agents_registry_store_name</code> matches the state store component defined in your <code>agentstate.yaml</code> file.</li> <li>Increment the <code>service_port</code> for each new agent service (e.g., 8001, 8002, 8003).</li> <li>Customize the Agent parameters (<code>role</code>, <code>name</code>, <code>goal</code>, and <code>instructions</code>) to match the behavior you want for each service.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#the-multi-app-run-template-file","title":"The Multi-App Run template file","text":"<p>The <code>Multi-App Run Template</code> File is a YAML configuration file named <code>dapr.yaml</code> that allows you to run multiple applications simultaneously. This file is placed at the same level as the <code>components/</code> and <code>services/</code> directories, ensuring a consistent and organized project structure.</p> <pre><code>dapr.yaml                  # The Multi-App Run template\ncomponents/                # Dapr configuration files\n\u251c\u2500\u2500 statestore.yaml        # State store configuration\n\u251c\u2500\u2500 pubsub.yaml            # Pub/Sub configuration\n\u2514\u2500\u2500 ...                    # Other Dapr components\nservices/                  # Directory for agent services\n\u251c\u2500\u2500 agent1/                # First agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent1\n\u2502   \u2514\u2500\u2500 ...                # Additional agent1 files\n\u2502\u2500\u2500 agent2/                # Second agent's service\n\u2502   \u251c\u2500\u2500 app.py             # FastAPI app for agent2\n\u2502   \u2514\u2500\u2500 ...                # Additional agent2 files\n\u2514\u2500\u2500 ...                    # More agents\n</code></pre> <p>Following our current scenario, we can set the following <code>Multi-App Run</code> template file:</p> <pre><code># https://docs.dapr.io/developing-applications/local-development/multi-app-dapr-run/multi-app-template/#template-properties\nversion: 1\ncommon:\n  resourcesPath: ./components\n  logLevel: info\n  appLogDestination: console\n  daprdLogDestination: console\n  configFilePath: config.yaml\n\napps:\n- appID: HobbitApp\n  appDirPath: ./services/hobbit/\n  appPort: 8001\n  command: [\"python3\", \"app.py\"]\n\n- appID: WizardApp\n  appDirPath: ./services/wizard/\n  appPort: 8002\n  command: [\"python3\", \"app.py\"]\n\n...\n\n- appID: RangerApp\n  appDirPath: ./services/ranger/\n  appPort: 8007\n  command: [\"python3\", \"app.py\"]\n\n- appID: WorkflowApp\n  appDirPath: ./services/workflow-llm/\n  command: [\"python3\", \"app.py\"]\n  appPort: 8009\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#starting-all-service-servers","title":"Starting All Service Servers","text":"<p>Tip</p> <p>Make sure you have your environment variables set up in an <code>.env</code> file so that the library can pick it up and use it to communicate with <code>OpenAI</code> services. We set them up in the LLM Inference Client section</p> <p>To start all the service servers defined in your project, you can use the <code>Dapr CLI</code> with the <code>Multi-App Run template</code> file. When you provide a directory path, the CLI will look for the <code>dapr.yaml</code> file (the default name for the template) in that directory. If the file is not found, the CLI will return an error.</p> <p>To execute the command, ensure you are in the root directory where the dapr.yaml file is located, then run:</p> <pre><code>dapr run -f .\n</code></pre> <p>This command reads the <code>dapr.yaml</code> file and starts all the services specified in the template.</p>"},{"location":"home/quickstarts/agentic_workflows/#monitor-services-initialization","title":"Monitor Services Initialization","text":"<ul> <li>Verify agent console logs: Each service outputs logs to confirm successful initialization.</li> </ul> <ul> <li>Verify orchestrator console logs: The workflow is initialized showing workflow and task registrations.</li> </ul> <ul> <li>Verify Redis entries: Access the Redis Insight interface at <code>http://localhost:5540/</code></li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#start-workflow-via-an-http-request","title":"Start Workflow via an HTTP Request","text":"<p>Once all services are running, you can initiate the workflow by making an HTTP POST request to the Agentic Workflow Service. This service orchestrates the workflow, triggering agent actions and handling communication among agents. The <code>.as_service(port=8004)</code> is required on the orchestrator to enable the HTTP endpoint and built-in <code>start-workflow</code> route.</p> <p>Here\u2019s an example of how to start the workflow using <code>curl</code>:</p> <pre><code>curl -i -X POST http://localhost:8009/start-workflow \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"task\": \"Lets solve the riddle to open the Doors of Durin and enter Moria.\"}'\n</code></pre> <pre><code>HTTP/1.1 200 OK\ndate: Sat, 22 Feb 2025 06:12:35 GMT\nserver: uvicorn\ncontent-length: 104\ncontent-type: application/json\n\n{\"message\":\"Workflow initiated successfully.\",\"workflow_instance_id\":\"8cd46d085d6a44fbb46e1c7c92abdd0f\"}\n</code></pre> <p>In this example:</p> <ul> <li>The request is sent to the Agentic Workflow Service running on port <code>8009</code>.</li> <li>The message parameter is passed as input to the <code>LLM Workflow</code>, which is then used to generate the plan and trigger the agentic workflow.</li> <li>This command demonstrates how to interact with the Agentic Workflow Service to kick off a new workflow.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#starting-the-workflow-by-publishing-a-triggeraction-message-optional","title":"Starting the Workflow by Publishing a <code>TriggerAction</code> Message (Optional)","text":"<p>Agentic workflows can also be triggered by publishing a message to the orchestrator's pub/sub topic. This is an optional method instead of making HTTP requests and enables fully message-driven coordination.</p>"},{"location":"home/quickstarts/agentic_workflows/#step-1-create-a-trigger-script","title":"Step 1: Create a Trigger Script","text":"<p>Create a Python file (e.g., trigger.py) in your <code>services/client/</code> directory with the following content:</p> <pre><code>#!/usr/bin/env python3\nimport json\nimport sys\nimport time\nimport argparse\nfrom dapr.clients import DaprClient\n\nPUBSUB_NAME = \"messagepubsub\"\n\ndef main(topic, max_attempts=10, retry_delay=1):\n    message = {\n        \"task\": \"How to get to Mordor? We all need to help!\"\n    }\n\n    time.sleep(5)  # Give orchestrators time to come online\n\n    for attempt in range(1, max_attempts + 1):\n        try:\n            print(f\"\ud83d\udce2 Attempt {attempt}: Publishing to topic '{topic}'...\")\n            with DaprClient() as client:\n                client.publish_event(\n                    pubsub_name=PUBSUB_NAME,\n                    topic_name=topic,\n                    data=json.dumps(message),\n                    data_content_type=\"application/json\",\n                    publish_metadata={\"cloudevent.type\": \"TriggerAction\"}\n                )\n            print(f\"\u2705 Message published to '{topic}'\")\n            sys.exit(0)\n        except Exception as e:\n            print(f\"\u274c Publish failed: {e}\")\n            if attempt &lt; max_attempts:\n                print(f\"\u23f3 Retrying in {retry_delay}s...\")\n                time.sleep(retry_delay)\n\n    print(\"\u274c Failed to publish message after multiple attempts.\")\n    sys.exit(1)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Trigger a workflow by publishing to a Dapr topic.\")\n    parser.add_argument(\"--orchestrator\", type=str, default=\"LLMOrchestrator\", help=\"Target orchestrator topic\")\n    args = parser.parse_args()\n    main(args.orchestrator)\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#step-2-run-the-trigger-script","title":"Step 2: Run the Trigger Script","text":"<p>Once all your services are running, run the script with the target orchestrator topic:</p> <pre><code>python3 trigger.py --orchestrator LLMOrchestrator\n</code></pre> <p>Or if you\u2019re running the <code>RandomOrchestrator</code> workflow:</p> <pre><code>python3 trigger.py --orchestrator RandomOrchestrator\n</code></pre> <p>This will publish a <code>TriggerAction</code> message to the orchestrator\u2019s topic, kicking off the workflow.</p> <p>In this example:</p> <ul> <li>The message is published to the orchestrator topic (e.g., <code>LLMOrchestrator</code>) via Dapr's Pub/Sub.</li> <li>The orchestrator service listens on its topic and receives the message of type <code>TriggerAction</code>.</li> <li>The message payload (e.g., a task) is used to generate the plan and initiate the agentic workflow.</li> <li>This approach is fully decoupled from HTTP\u2014no more direct POST requests to a service endpoint.</li> <li>It enables truly asynchronous and event-driven orchestration, making your system more scalable and resilient.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#monitoring-workflow-execution","title":"Monitoring Workflow Execution","text":"<ul> <li>Check console logs to trace activities in the workflow.</li> </ul> <ul> <li>Verify Redis entries: Access the Redis Insight interface at <code>http://localhost:5540/</code></li> </ul> <ul> <li>You can also check the <code>Workflow State</code> in the Redis Insight interface at <code>http://localhost:5540</code>. You can click on it, copy the log entry and paste it in your favorite editor. It is a <code>JSON</code> object. You will be able to see the chat history, the plan and tasks being completed.</li> </ul> <ul> <li>As mentioned earlier, when we ran dapr init, Dapr initialized, a <code>Zipkin</code> container instance, used for observability and tracing. Open <code>http://localhost:9411/zipkin/</code> in your browser to view traces &gt; Find a Trace &gt; Run Query.</li> </ul> <ul> <li>Select the trace entry with multiple spans labeled <code>&lt;workflow name&gt;: /taskhubsidecarservice/startinstance.</code>. When you open this entry, you\u2019ll see details about how each task or activity in the workflow was executed. If any task failed, the error will also be visible here.</li> </ul> <ul> <li>Check console logs to validate if workflow was executed successfuly.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#switching-orchestrator","title":"Switching Orchestrator","text":"<p>You can easily switch to a different <code>Orchestrator</code> type by updating the <code>dapr.yaml</code> file.</p>"},{"location":"home/quickstarts/agentic_workflows/#available-workflow-options","title":"Available Workflow Options","text":"<ul> <li>RoundRobin: Cycles through agents in a fixed order, ensuring each agent gets an equal opportunity to process tasks.</li> <li>Random: Selects an agent randomly for each task.</li> <li>LLM-based: Uses a large language model (e.g., GPT-4o) to determine the most suitable agent based on the message and context.</li> </ul>"},{"location":"home/quickstarts/agentic_workflows/#switching-to-the-random-workflow","title":"Switching to the Random Workflow","text":"<ul> <li>Update dapr.yaml: Modify the appDirPath for the workflow service to point to the <code>workflow-random</code> directory:</li> </ul> <pre><code>- appID: WorkflowApp\n  appDirPath: ./services/workflow-random/\n  command: [\"python3\", \"app.py\"]\n</code></pre>"},{"location":"home/quickstarts/agentic_workflows/#reset-redis-database","title":"Reset Redis Database","text":"<ol> <li>Access the Redis Insight interface at <code>http://localhost:5540/</code></li> <li>In the search bar type <code>*</code> to select all items in the database.</li> <li>Click on <code>Bulk Actions</code> &gt; <code>Delete</code> &gt; <code>Delete</code></li> </ol> <p>You should see an empty database now:</p> <p></p>"},{"location":"home/quickstarts/agentic_workflows/#testing-new-workflow","title":"Testing New Workflow","text":"<p>Restart the services with <code>dapr run -f</code> . and send a message to the workflow. Always ensure your <code>.env</code> file is configured correctly and contains the necessary credentials if needed.</p>"}]}